{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ XGBoost: Detecci√≥n de Fraude\n",
        "\n",
        "**Materia:** Ciencia de Datos  \n",
        "**Autores:** Mariana P√©rez P√©rez, Yoriel Navier Carvajalino Vidal, Adriana Lucia Castro Carre√±o\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Objetivos de esta demostraci√≥n:\n",
        "\n",
        "1. ‚úÖ Aplicar XGBoost a un problema real de clasificaci√≥n\n",
        "2. üîç **Demostrar c√≥mo XGBoost selecciona variables** (Gain, Weight, Cover)\n",
        "3. üìä **Visualizar el proceso iterativo de aprendizaje**\n",
        "4. ‚öñÔ∏è **Mostrar el efecto de la regularizaci√≥n** (L1 y L2)\n",
        "5. üé® **Ilustrar splits y thresholds** en la toma de decisiones\n",
        "6. üìà Evaluar el rendimiento con m√©tricas apropiadas\n"
      ],
      "metadata": {
        "id": "4cc2owZVYmUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Paso 1: Configuraci√≥n del Entorno\n",
        "\n",
        "Instalamos e importamos todas las librer√≠as necesarias."
      ],
      "metadata": {
        "id": "W6O94JBrZZYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalaci√≥n de librer√≠as\n",
        "!pip install -q gdown xgboost\n",
        "\n",
        "# Importaciones\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "import os\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    precision_recall_curve, \n",
        "    auc,\n",
        "    roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Configuraciones\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
        "print(f\"üìå Versi√≥n de XGBoost: {xgb.__version__}\")"
      ],
      "metadata": {
        "id": "gpSdsTmGZdKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Paso 2: Carga y An√°lisis Exploratorio de Datos\n",
        "\n",
        "Cargamos el dataset de detecci√≥n de fraude en tarjetas de cr√©dito."
      ],
      "metadata": {
        "id": "MYDqSiuvaCYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga autom√°tica del dataset\n",
        "output_filename = \"creditcard.csv\"\n",
        "\n",
        "if not os.path.exists(output_filename):\n",
        "    print(f\"üì• Descargando dataset...\")\n",
        "    file_id = \"1JXhUJjoGnRBR6tUkvvPv8DFisZZI-Iwc\"\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(url, output_filename, quiet=False)\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset ya existe\")\n",
        "\n",
        "# Cargar datos\n",
        "df = pd.read_csv(output_filename)\n",
        "print(f\"\\nüìê Dimensiones: {df.shape}\")\n",
        "print(f\"üìä Columnas: {df.shape[1]}\")\n",
        "print(f\"üìù Registros: {df.shape[0]:,}\")\n",
        "\n",
        "# Mostrar primeras filas\n",
        "display(df.head())\n",
        "\n",
        "# Informaci√≥n del dataset\n",
        "print(\"\\nüìã Informaci√≥n del dataset:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "TU8NjhOpaErP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç An√°lisis del Desbalance de Clases\n",
        "\n",
        "Este es un dataset **altamente desbalanceado**, caracter√≠stica com√∫n en problemas de detecci√≥n de fraude."
      ],
      "metadata": {
        "id": "analysis_balance"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An√°lisis del desbalance\n",
        "class_counts = df['Class'].value_counts()\n",
        "class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"üéØ Distribuci√≥n de clases:\")\n",
        "print(f\"   No Fraude (0): {class_counts[0]:,} ({class_percentages[0]:.2f}%)\")\n",
        "print(f\"   Fraude (1):    {class_counts[1]:,} ({class_percentages[1]:.3f}%)\")\n",
        "print(f\"\\n‚öñÔ∏è Ratio de desbalance: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gr√°fico de barras\n",
        "class_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Distribuci√≥n de Clases (Cantidad)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Clase')\n",
        "axes[0].set_ylabel('Cantidad')\n",
        "axes[0].set_xticklabels(['No Fraude', 'Fraude'], rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Gr√°fico de pie\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "axes[1].pie(class_counts, labels=['No Fraude', 'Fraude'], autopct='%1.3f%%', \n",
        "            colors=colors, startangle=90, textprops={'fontsize': 12})\n",
        "axes[1].set_title('Proporci√≥n de Clases', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Implicaci√≥n: Necesitamos usar scale_pos_weight para manejar el desbalance\")"
      ],
      "metadata": {
        "id": "balance_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Paso 3: Preprocesamiento de Datos\n",
        "\n",
        "Preparamos los datos para el modelado."
      ],
      "metadata": {
        "id": "preprocessing"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar caracter√≠sticas y variable objetivo\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Estandarizaci√≥n de Amount y Time\n",
        "scaler = StandardScaler()\n",
        "X['Amount'] = scaler.fit_transform(X[['Amount']])\n",
        "X['Time'] = scaler.fit_transform(X[['Time']])\n",
        "\n",
        "print(\"‚úÖ Datos estandarizados\")\n",
        "print(f\"üìä Features: {X.shape[1]}\")\n",
        "print(f\"üéØ Samples: {X.shape[0]:,}\")\n",
        "\n",
        "# Divisi√≥n train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nüìö Train set: {X_train.shape[0]:,} muestras\")\n",
        "print(f\"üß™ Test set:  {X_test.shape[0]:,} muestras\")"
      ],
      "metadata": {
        "id": "preprocessing_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Paso 4: Entrenamiento de Modelos XGBoost\n",
        "\n",
        "### üéØ 4.1: Modelo Base (Con Regularizaci√≥n)\n",
        "\n",
        "Entrenamos un modelo con regularizaci√≥n para demostrar los conceptos del PDF."
      ],
      "metadata": {
        "id": "training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular scale_pos_weight para el desbalance\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"‚öñÔ∏è scale_pos_weight calculado: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# Modelo con regularizaci√≥n (como explica el PDF)\n",
        "model_regularized = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    # üî• Regularizaci√≥n (L1 y L2 como en el PDF)\n",
        "    reg_alpha=0.1,   # L1 regularization (Lasso)\n",
        "    reg_lambda=1.0,  # L2 regularization (Ridge)\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "\n",
        "# Entrenamiento con seguimiento del error\n",
        "print(\"\\nüéØ Entrenando modelo con regularizaci√≥n...\")\n",
        "model_regularized.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo entrenado exitosamente\")"
      ],
      "metadata": {
        "id": "training_regularized"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä 4.2: Visualizaci√≥n del Proceso Iterativo de Aprendizaje\n",
        "\n",
        "**Como explica el PDF:** XGBoost aprende iterativamente, reduciendo el error en cada paso."
      ],
      "metadata": {
        "id": "iterative_learning"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener resultados de evaluaci√≥n por iteraci√≥n\n",
        "results = model_regularized.evals_result()\n",
        "\n",
        "# Crear visualizaci√≥n\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot para train y test\n",
        "epochs = range(len(results['validation_0']['logloss']))\n",
        "ax.plot(epochs, results['validation_0']['logloss'], \n",
        "        label='Train Error', linewidth=2, color='#3498db')\n",
        "ax.plot(epochs, results['validation_1']['logloss'], \n",
        "        label='Test Error', linewidth=2, color='#e74c3c')\n",
        "\n",
        "ax.set_xlabel('N√∫mero de √Årboles (Iteraciones)', fontsize=12)\n",
        "ax.set_ylabel('Log Loss (Error)', fontsize=12)\n",
        "ax.set_title('üìâ Proceso Iterativo de Aprendizaje de XGBoost\\n(Cada √°rbol corrige errores del anterior)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Anotar puntos clave\n",
        "min_idx = np.argmin(results['validation_1']['logloss'])\n",
        "ax.annotate(f'Mejor modelo\\n(√Årbol {min_idx})', \n",
        "            xy=(min_idx, results['validation_1']['logloss'][min_idx]),\n",
        "            xytext=(min_idx+10, results['validation_1']['logloss'][min_idx]+0.01),\n",
        "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
        "            fontsize=10, color='green', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüí° Observaci√≥n: El error disminuye con cada iteraci√≥n\")\n",
        "print(f\"   Mejor modelo en el √°rbol #{min_idx}\")\n",
        "print(f\"   Error final (test): {results['validation_1']['logloss'][-1]:.6f}\")"
      ],
      "metadata": {
        "id": "iterative_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç 4.3: Comparaci√≥n - Efecto de la Regularizaci√≥n\n",
        "\n",
        "**Como explica el PDF:** La regularizaci√≥n (L1 y L2) previene el sobreajuste."
      ],
      "metadata": {
        "id": "regularization_comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo SIN regularizaci√≥n para comparar\n",
        "print(\"üéØ Entrenando modelo SIN regularizaci√≥n (para comparar)...\")\n",
        "model_no_reg = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    # ‚ùå SIN regularizaci√≥n\n",
        "    reg_alpha=0,\n",
        "    reg_lambda=0,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "\n",
        "model_no_reg.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ambos modelos listos para comparar\")\n",
        "\n",
        "# Comparaci√≥n visual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Modelo CON regularizaci√≥n\n",
        "results_reg = model_regularized.evals_result()\n",
        "epochs = range(len(results_reg['validation_0']['logloss']))\n",
        "axes[0].plot(epochs, results_reg['validation_0']['logloss'], \n",
        "             label='Train', linewidth=2, color='#3498db')\n",
        "axes[0].plot(epochs, results_reg['validation_1']['logloss'], \n",
        "             label='Test', linewidth=2, color='#e74c3c')\n",
        "axes[0].set_xlabel('Iteraciones', fontsize=11)\n",
        "axes[0].set_ylabel('Error (Log Loss)', fontsize=11)\n",
        "axes[0].set_title('‚úÖ CON Regularizaci√≥n (L1 + L2)\\nModelo m√°s robusto', \n",
        "                  fontsize=12, fontweight='bold', color='green')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Modelo SIN regularizaci√≥n\n",
        "results_no_reg = model_no_reg.evals_result()\n",
        "axes[1].plot(epochs, results_no_reg['validation_0']['logloss'], \n",
        "             label='Train', linewidth=2, color='#3498db')\n",
        "axes[1].plot(epochs, results_no_reg['validation_1']['logloss'], \n",
        "             label='Test', linewidth=2, color='#e74c3c')\n",
        "axes[1].set_xlabel('Iteraciones', fontsize=11)\n",
        "axes[1].set_ylabel('Error (Log Loss)', fontsize=11)\n",
        "axes[1].set_title('‚ö†Ô∏è SIN Regularizaci√≥n\\nPosible sobreajuste', \n",
        "                  fontsize=12, fontweight='bold', color='red')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lisis de overfitting\n",
        "train_error_reg = results_reg['validation_0']['logloss'][-1]\n",
        "test_error_reg = results_reg['validation_1']['logloss'][-1]\n",
        "gap_reg = abs(train_error_reg - test_error_reg)\n",
        "\n",
        "train_error_no_reg = results_no_reg['validation_0']['logloss'][-1]\n",
        "test_error_no_reg = results_no_reg['validation_1']['logloss'][-1]\n",
        "gap_no_reg = abs(train_error_no_reg - test_error_no_reg)\n",
        "\n",
        "print(\"\\nüìä An√°lisis de Sobreajuste (Overfitting):\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Modelo':<25} {'Train Error':<15} {'Test Error':<15} {'Gap':<10}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'CON Regularizaci√≥n':<25} {train_error_reg:<15.6f} {test_error_reg:<15.6f} {gap_reg:<10.6f}\")\n",
        "print(f\"{'SIN Regularizaci√≥n':<25} {train_error_no_reg:<15.6f} {test_error_no_reg:<15.6f} {gap_no_reg:<10.6f}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüí° Gap menor = Mejor generalizaci√≥n\")\n",
        "print(f\"   ‚úÖ Modelo con regularizaci√≥n: {gap_reg:.6f}\")\n",
        "print(f\"   ‚ö†Ô∏è  Modelo sin regularizaci√≥n: {gap_no_reg:.6f}\")"
      ],
      "metadata": {
        "id": "regularization_comparison_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Paso 5: Selecci√≥n y Evaluaci√≥n de Variables\n",
        "\n",
        "### üìä 5.1: Feature Importance - Las 3 M√©tricas de XGBoost\n",
        "\n",
        "**Como explica el PDF**, XGBoost usa 3 m√©tricas para evaluar variables:\n",
        "- **Gain (Ganancia):** Cu√°nto mejora el modelo al usar esa variable\n",
        "- **Weight (Peso):** N√∫mero de veces que aparece en las divisiones\n",
        "- **Cover (Cobertura):** Cantidad de muestras afectadas"
      ],
      "metadata": {
        "id": "feature_importance"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear figura con 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# 1. GAIN (Ganancia) - La m√°s importante\n",
        "xgb.plot_importance(model_regularized, importance_type='gain', \n",
        "                   ax=axes[0], max_num_features=10, \n",
        "                   title='', color='#3498db')\n",
        "axes[0].set_title('üìà GAIN (Ganancia)\\nCu√°nto mejora el modelo', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Gain Score', fontsize=10)\n",
        "\n",
        "# 2. WEIGHT (Peso)\n",
        "xgb.plot_importance(model_regularized, importance_type='weight', \n",
        "                   ax=axes[1], max_num_features=10,\n",
        "                   title='', color='#e74c3c')\n",
        "axes[1].set_title('‚öñÔ∏è WEIGHT (Peso)\\nFrecuencia de uso', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Weight Score', fontsize=10)\n",
        "\n",
        "# 3. COVER (Cobertura)\n",
        "xgb.plot_importance(model_regularized, importance_type='cover', \n",
        "                   ax=axes[2], max_num_features=10,\n",
        "                   title='', color='#2ecc71')\n",
        "axes[2].set_title('üìä COVER (Cobertura)\\nMuestras afectadas', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('Cover Score', fontsize=10)\n",
        "\n",
        "plt.suptitle('üîç Importancia de Variables en XGBoost - Las 3 M√©tricas', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Obtener las variables m√°s importantes por GAIN\n",
        "importance_dict = model_regularized.get_booster().get_score(importance_type='gain')\n",
        "importance_sorted = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nüèÜ TOP 10 Variables M√°s Importantes (por GAIN):\")\n",
        "print(\"=\"*50)\n",
        "for i, (feature, score) in enumerate(importance_sorted[:10], 1):\n",
        "    print(f\"{i:2d}. {feature:<8} ‚Üí Gain: {score:>10.2f}\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nüí° Estas variables tienen el mayor impacto en las predicciones\")"
      ],
      "metadata": {
        "id": "feature_importance_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üå≥ 5.2: Visualizaci√≥n de un √Årbol - Splits y Thresholds\n",
        "\n",
        "**Como explica el PDF:** XGBoost decide mediante splits (divisiones) y thresholds (umbrales)."
      ],
      "metadata": {
        "id": "tree_visualization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar el primer √°rbol del modelo\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "xgb.plot_tree(model_regularized, num_trees=0, ax=ax)\n",
        "plt.title('üå≥ Visualizaci√≥n de un √Årbol de Decisi√≥n en XGBoost\\nCada nodo muestra: [Variable < Threshold]', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° En cada nodo del √°rbol:\")\n",
        "print(\"   - Se selecciona una variable (ej: 'V14')\")\n",
        "print(\"   - Se define un threshold (ej: < -1.5)\")\n",
        "print(\"   - Se divide la data seg√∫n esa condici√≥n\")\n",
        "print(\"   - XGBoost elige el split con mayor GAIN\")"
      ],
      "metadata": {
        "id": "tree_viz_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Paso 6: Evaluaci√≥n del Modelo\n",
        "\n",
        "### üéØ 6.1: Matriz de Confusi√≥n"
      ],
      "metadata": {
        "id": "evaluation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones\n",
        "y_pred = model_regularized.predict(X_test)\n",
        "y_pred_proba = model_regularized.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizaci√≥n mejorada\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Fraude', 'Fraude'],\n",
        "            yticklabels=['No Fraude', 'Fraude'],\n",
        "            cbar_kws={'label': 'Cantidad'},\n",
        "            annot_kws={'size': 16, 'weight': 'bold'})\n",
        "plt.title('üìä Matriz de Confusi√≥n', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Valor Real', fontsize=12)\n",
        "plt.xlabel('Predicci√≥n', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretaci√≥n\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"\\nüîç Interpretaci√≥n de la Matriz:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Verdaderos Negativos (TN): {tn:>6,} - Correctamente identificados como NO fraude\")\n",
        "print(f\"‚ùå Falsos Positivos (FP):     {fp:>6,} - Incorrectamente marcados como fraude\")\n",
        "print(f\"‚ùå Falsos Negativos (FN):     {fn:>6,} - Fraudes no detectados\")\n",
        "print(f\"‚úÖ Verdaderos Positivos (TP): {tp:>6,} - Fraudes correctamente detectados\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä 6.2: Reporte de Clasificaci√≥n\n",
        "\n",
        "M√©tricas detalladas de rendimiento."
      ],
      "metadata": {
        "id": "classification_report"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reporte de clasificaci√≥n\n",
        "print(\"\\nüìä REPORTE DE CLASIFICACI√ìN\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(y_test, y_pred, \n",
        "                          target_names=['No Fraude', 'Fraude'],\n",
        "                          digits=4))\n",
        "\n",
        "# M√©tricas adicionales\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"\\nüéØ M√âTRICAS GLOBALES:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Accuracy (Exactitud):     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision (Precisi√≥n):    {precision:.4f} ({precision*100:.2f}%)\")\n",
        "print(f\"Recall (Sensibilidad):    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "print(f\"F1-Score:                 {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score:            {roc_auc:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüí° Interpretaci√≥n:\")\n",
        "print(f\"   ‚úÖ De cada 100 predicciones de fraude, {precision*100:.1f} son correctas (Precision)\")\n",
        "print(f\"   ‚úÖ Detectamos el {recall*100:.1f}% de todos los fraudes reales (Recall)\")\n",
        "print(f\"   ‚úÖ Balance entre ambas: {f1:.4f} (F1-Score)\")"
      ],
      "metadata": {
        "id": "classification_report_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìà 6.3: Curvas ROC y Precision-Recall"
      ],
      "metadata": {
        "id": "curves"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular m√©tricas para las curvas\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
        "pr_auc = auc(recall_vals, precision_vals)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Curva ROC\n",
        "axes[0].plot(fpr, tpr, color='#3498db', linewidth=2, \n",
        "            label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1, label='Random')\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[0].set_title('üìà Curva ROC (Receiver Operating Characteristic)', \n",
        "                 fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Curva Precision-Recall\n",
        "axes[1].plot(recall_vals, precision_vals, color='#e74c3c', linewidth=2,\n",
        "            label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
        "axes[1].set_xlabel('Recall (Sensibilidad)', fontsize=12)\n",
        "axes[1].set_ylabel('Precision (Precisi√≥n)', fontsize=12)\n",
        "axes[1].set_title('üìä Curva Precision-Recall', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Resultados de las Curvas:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ROC-AUC:           {roc_auc:.4f} (Excelente si > 0.90)\")\n",
        "print(f\"Precision-Recall:  {pr_auc:.4f} (Excelente si > 0.80)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüí° La curva PR es m√°s informativa para datasets desbalanceados\")"
      ],
      "metadata": {
        "id": "curves_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéì Conclusiones\n",
        "\n",
        "### ‚úÖ Lo que demostramos en este notebook:\n",
        "\n",
        "1. **üìä Aprendizaje Iterativo:**\n",
        "   - Visualizamos c√≥mo el error disminuye con cada √°rbol\n",
        "   - Cada modelo aprende de los errores del anterior\n",
        "\n",
        "2. **‚öñÔ∏è Regularizaci√≥n:**\n",
        "   - Comparamos modelos con y sin regularizaci√≥n\n",
        "   - Demostramos c√≥mo L1 y L2 previenen el sobreajuste\n",
        "\n",
        "3. **üîç Selecci√≥n de Variables:**\n",
        "   - Mostramos las 3 m√©tricas: Gain, Weight, Cover\n",
        "   - Identificamos las variables m√°s importantes\n",
        "\n",
        "4. **üå≥ Splits y Thresholds:**\n",
        "   - Visualizamos c√≥mo el modelo toma decisiones\n",
        "   - Cada nodo representa una divisi√≥n basada en un umbral\n",
        "\n",
        "5. **üìà Evaluaci√≥n Completa:**\n",
        "   - Matriz de confusi√≥n\n",
        "   - M√©tricas apropiadas para datos desbalanceados\n",
        "   - Curvas ROC y Precision-Recall\n",
        "\n",
        "### üéØ Resultados Finales:\n",
        "\n",
        "XGBoost demostr√≥ ser altamente efectivo para detectar fraude:\n",
        "- ‚úÖ Alta precisi√≥n en la detecci√≥n de fraudes\n",
        "- ‚úÖ Manejo efectivo del desbalanceo de clases\n",
        "- ‚úÖ Selecci√≥n autom√°tica de variables importantes\n",
        "- ‚úÖ Prevenci√≥n de sobreajuste mediante regularizaci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "**üí° XGBoost es uno de los algoritmos m√°s utilizados en la Ciencia de Datos moderna gracias a su:**\n",
        "- üöÄ Alta precisi√≥n\n",
        "- ‚ö° Velocidad de entrenamiento\n",
        "- üõ°Ô∏è Robustez ante sobreajuste\n",
        "- üîç Capacidad de selecci√≥n de variables"
      ],
      "metadata": {
        "id": "conclusions"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìö Referencias\n",
        "\n",
        "- Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System\n",
        "- Documentaci√≥n oficial de XGBoost: https://xgboost.readthedocs.io/\n",
        "- Dataset: Credit Card Fraud Detection (Kaggle)\n",
        "\n",
        "---\n",
        "\n",
        "**Creado por:** Mariana P√©rez P√©rez, Yoriel Navier Carvajalino Vidal, Adriana Lucia Castro Carre√±o  \n",
        "**Materia:** Ciencia de Datos  \n",
        "**Fecha:** 2024"
      ],
      "metadata": {
        "id": "references"
      }
    }
  ]
}
